#!/bin/bash
cd swarm/

sudo docker swarm init
#join worker nodes to the swarm with output generated by previous command

sudo docker network create -d overlay --attachable core
sudo docker stack deploy -c docker-compose-hadoop-swarm.yml hadoop
#check if namenode is created before of datanode
#if not follow instructions of this README https://github.com/big-data-europe/docker-hadoop-spark-workbench/tree/master/swarm
sudo docker stack deploy -c docker-compose-spark.yml spark

#sudo docker service ls		#check active services in the swarm
#sudo docker container ls	#check active containers in the node