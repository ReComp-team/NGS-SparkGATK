The analysis performance conducted in the final chapter was conducted using different realeases of the GATK 4.0 version. This led to pretty different execution times for the same GATK tool, according to the release version used. So it is valuable to re-conduct these analysis performance when there will be a stable version of these tools. In particular for cluster-mode execution, since that in the last GATK release there are some problems in executing HaplotypeCallerSpark.
\\[1\baselineskip]
As exposed in previous chapters, Sparkifed tools actually are implemented to be executed on a single machine, since a bash script must be generated on the local file system. It could be implemented in cluster-mode, maybe trying to generate this script on HDFS and so all the Spark Workers would be able to access to the bash script, due to execute it in distributed mode through Spark.
\\[1\baselineskip]
The final output result generated from the pipeline is reliable, since compared with the result generated from an other pipeline \cite{ScalableEfficientWhole-ExomeProcessing}. But there are some differences and things to do:
\begin{itemize}
	\item Files generated by this pipeline are nearly four times larger than what the e-SC pipeline returns, but that is mainly because in this pipeline was not filtered VCF by quality. In the e-SC pipeline was used Phred=30. It should be filter out the variants that have quality less than 30.
    
    \item There are some extra rows which the other pipeline filter out like 'chrM', 'chr1\_gl000...', 'chrUn\_gl000...'. The first is the mitochondrial DNA, the other are some unknown reads, possibly coming from sample contamination. These may be filtered too.
    
    \item There are some redundant columns in output, e.g. column 38 is exactly the same as column 11 (both include some ensGene references like ENSG00000198888), column 17 includes only dots '.'
\end{itemize}

When executed in cluster-mode, as exposed in chapter Clustering, after converting the input FASTQ samples through \textit{FastqToSam}, is then necessary to load the generated outputs in HDFS (within other files). Instead of waiting the output generated from the tool, maybe is possible to put it in a \textit{named pipe}, which will be the input for the HDFS loading command. In this way probably the overhead of loading the converted samples in HDFS could be avoided, so is valuable to explore this approach.
\\[1\baselineskip]
HaplotypeCallerSpark presented unexpected execution times when scaling up or out, because of this warning:

\textit{Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS\_CACHING implementation !}

Maybe it is due to use of GATK 4.0.2 release or maybe for some hardware alteration; anyway this led to unexpected results. It is interesting to understand the reason of this warning and attempt to resolve it in order to report correct execution times.
\\[1\baselineskip]
Execution of GATK Spark tools on a 32 cores VM did not lead to any speed-up, while executing on 2 VM with 16 cores or 3 VM with 8 cores led to further speed-up instead. It is interesting to understand the reason why a 32 cores VM did not lead speed-up.










